{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = pd.read_csv(\"data/four_area/author.txt\", sep = \"\\t\", names=[\"ID\", \"Author name\"],encoding='utf8')\n",
    "conf_df = pd.read_csv(\"data/four_area/conf.txt\", sep = \"\\t\", names=[\"ID\", \"Conference name\"])\n",
    "paper_df = pd.read_csv(\"data/four_area/paper.txt\", sep = \"\\t\", names=[\"ID\", \"Paper title\"])\n",
    "term_df = pd.read_csv(\"data/four_area/term.txt\", sep = \"\\t\", names=[\"ID\", \"Term\"])\n",
    "paper_author = pd.read_csv(\"data/four_area/paper_author.txt\", sep = \"\\t\", names=[\"paperID\", \"authorID\"])\n",
    "paper_conf = pd.read_csv(\"data/four_area/paper_conf.txt\", sep = \"\\t\", names=[\"paperID\", \"confID\"])\n",
    "paper_term = pd.read_csv(\"data/four_area/paper_term.txt\", sep = \"\\t\", names=[\"paperID\", \"termID\"])\n",
    "author_dict = pd.read_csv(\"data/DBLP_four_area/cleaned_author_dict.txt\", sep = \"\\t\", names=[\"ID\", \"Author name\"], encoding='utf8')\n",
    "conf_dict = pd.read_csv(\"data/DBLP_four_area/conf_dict.txt\", sep = \"\\t\", names=[\"ID\", \"Conference name\"])\n",
    "term_dict = pd.read_csv(\"data/DBLP_four_area/term_dict.txt\", sep = \"\\t\", names=[\"ID\", \"Term\"])\n",
    "author_label = pd.read_csv(\"data/DBLP_four_area/author_label.txt\", sep = \"\\t\", names=[\"authorID\", \"Label\"])\n",
    "conf_label = pd.read_csv(\"data/DBLP_four_area/conf_label.txt\", sep = \"\\t\", names=[\"confID\", \"Conference name\", \"Label\"])\n",
    "conf_dict_m = pd.merge(conf_dict, conf_df, on='Conference name')\n",
    "author_dict_m = pd.merge(author_dict, author_df, on='Author name')\n",
    "paper_conf_m = pd.merge(conf_dict_m, paper_conf, left_on='ID_y', right_on='confID')\n",
    "paper_conf_m = paper_conf_m.drop(columns=['Conference name', 'ID_y','confID'])\n",
    "paper_label_m = pd.merge(paper_conf_m, conf_label, left_on='ID_x', right_on='confID')\n",
    "paper_label_m = paper_label_m.drop(columns=['Conference name', 'ID_x','confID'])\n",
    "author_paper_label_m = pd.merge(paper_label_m,paper_author,on='paperID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for author in tqdm(author_paper_label_m['authorID'].unique()):\n",
    "#     author_dict_ID = int(author_dict_m[author_dict_m[\"ID_y\"] == author]['ID_x'].to_string(index=False).strip())\n",
    "#     value_count = author_paper_label_m[author_paper_label_m['authorID'] == author]['Label'].value_counts()\n",
    "#     for vc in value_count.iteritems():\n",
    "#         label = vc[0]\n",
    "#         count = vc[1]\n",
    "#         author_feature.at[author_dict_ID, label]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_feature = pd.read_pickle(r'F:\\author_feature.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features = author_feature.loc[author_label['authorID'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_sum = np.sum(author_feature, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features['sum'] = author_sum\n",
    "author_features['lgt_median'] = author_sum > 2\n",
    "author_features['lgt_mean'] = author_sum > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features['DB_lgt_0'] = author_features[1]  > 0\n",
    "author_features['DM_lgt_0'] = author_features[2]  > 0\n",
    "author_features['ML_lgt_0'] = author_features[3]  > 0\n",
    "author_features['IR_lgt_0'] = author_features[4]  > 0\n",
    "\n",
    "author_features['DB_lgt_0.25'] = author_features[1] / author_features['sum'] > 0.25\n",
    "author_features['DM_lgt_0.25'] = author_features[2] / author_features['sum'] > 0.25\n",
    "author_features['ML_lgt_0.25'] = author_features[3] / author_features['sum'] > 0.25\n",
    "author_features['IR_lgt_0.25'] = author_features[4] / author_features['sum'] > 0.25\n",
    "\n",
    "author_features['co_1'] = 0\n",
    "author_features['co_2'] = 0    \n",
    "author_features['co_3'] = 0    \n",
    "author_features['co_4'] = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb57d56d29654a788de27d641d791b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(author_features.iterrows()):\n",
    "    adid = index\n",
    "    aid = int(author_dict_m[author_dict_m[\"ID_x\"] == adid]['ID_y'].to_string(index=False).strip())\n",
    "    all_paper = paper_author[paper_author['authorID'] == aid]['paperID']\n",
    "    coauthor = []\n",
    "    for paper in all_paper:\n",
    "        authors = paper_author[paper_author['paperID'] == paper]['authorID']\n",
    "        authors = authors[authors != aid]\n",
    "        coauthor = np.concatenate((coauthor, authors))\n",
    "    coauthor = np.unique(coauthor)\n",
    "    coauthor_hashed = author_dict_m[author_dict_m['ID_y'].isin(coauthor)]['ID_x']\n",
    "    vc = author_label[author_label['authorID'].isin(coauthor_hashed)]['Label'].value_counts()\n",
    "    author_features.loc[adid, 'co_1']  += vc[1] if 1 in vc.index else 0\n",
    "    author_features.loc[adid, 'co_2']  += vc[2] if 2 in vc.index else 0    \n",
    "    author_features.loc[adid, 'co_3']  += vc[3] if 3 in vc.index else 0    \n",
    "    author_features.loc[adid, 'co_4']  += vc[4] if 4 in vc.index else 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features['DB_co'] = author_features['co_1'] > 0\n",
    "author_features['DM_co'] = author_features['co_2'] > 0    \n",
    "author_features['ML_co'] = author_features['co_3'] > 0    \n",
    "author_features['IR_co'] = author_features['co_4'] > 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features = pd.merge(author_features, author_label, left_index = True, right_on='authorID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features = pd.read_pickle(r'F:\\author_features.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables of the MRF: \n",
    "1. Number of paper by author > 2 (median)\n",
    "2. Number of paper by author > 5 (mean)\n",
    "7. Percentage of DB paper > 0.25\n",
    "8. Percentage of DM paper > 0.25\n",
    "9. Percentage of ML paper > 0.25\n",
    "10. Percentage of IR paper > 0.25\n",
    "11. Co-authored with DB \n",
    "12. Co-authored with DM \n",
    "13. Co-authored with ML \n",
    "14. Co-authored with IR\n",
    "\n",
    "3. ~~Number of DB paper > 0~~\n",
    "4. ~~Number of DM paper > 0~~\n",
    "5. ~~Number of ML paper > 0~~\n",
    "6. ~~Number of IR paper > 0~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = author_features[['lgt_median', 'lgt_mean', 'DB_lgt_0', 'DM_lgt_0', 'ML_lgt_0', 'IR_lgt_0',\n",
    "                'DB_lgt_0.25', 'DM_lgt_0.25', 'ML_lgt_0.25', 'IR_lgt_0.25', \n",
    "                'DB_co', 'DM_co', 'ML_co', 'IR_co']]\n",
    "y = author_features[['Label']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_actual = []\n",
    "y_train_actual = []\n",
    "X_test_actual = []\n",
    "y_test_actual = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in X_train.iterrows():\n",
    "    X_train_actual.append([row.to_dict()])\n",
    "for i, row in X_test.iterrows():\n",
    "    X_test_actual.append([row.to_dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in y_train.iterrows():\n",
    "    y_train_actual.append([row.to_string(index = False).strip()])\n",
    "for i, row in y_test.iterrows():\n",
    "    y_test_actual.append([row.to_string(index = False).strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
       "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
       "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
       "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.fit(X_train_actual, y_train_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142716068506009"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test_actual)\n",
    "metrics.flat_f1_score(y_test_actual, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
